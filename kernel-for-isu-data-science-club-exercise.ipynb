{"cells":[{"metadata":{"_uuid":"7f1e7bd1f1936fc2a0e0bb413b271765fa3733e2"},"cell_type":"markdown","source":"This notebook has most of the lessons found on KUC lesson implemented. Do your best to improve the final result of this notebook. If you have any questions about what this notebook is doing, [please look at the lesson that covers it](http://https://www.kaggle.com/learn/machine-learning) or ask me. Let's start by loading our data:"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom sklearn.tree import DecisionTreeRegressor\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7096563486bdea61055af9a644a6839f5870a145","scrolled":true},"cell_type":"code","source":"training_data = pd.read_csv('../input/train.csv')\ntest_data = pd.read_csv('../input/test.csv')\ntraining_data.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2df558b4caa0801d3c1fd372df2ae42d7fd18ee0"},"cell_type":"markdown","source":"Now that we have our data, we need to get it in a format we can use.\n\nFirst, we drop anything that isn't a number, such as text and NaN values.\n\nNext, we separate the variable we want to predict from the rest of our variables, then drop that column.\n\nOptionally, we can take a look at our statistics about the data we have left."},{"metadata":{"trusted":true,"_uuid":"897db0ed815a54e0ca8ee09d6a4ae52460b9cf7f"},"cell_type":"code","source":"X = training_data.select_dtypes(exclude='object')\nX = X.dropna()\n\ny = X.SalePrice\n\nX = X.drop(labels=\"SalePrice\", axis=1)\nX.describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"02aa839088093a7b7e1989c8f2295ccca94c8958"},"cell_type":"markdown","source":"Now, we create our model."},{"metadata":{"trusted":true,"_uuid":"b2a014d76911702af5a1fd20d14ccfd4429bd014","scrolled":true},"cell_type":"code","source":"ames_model = DecisionTreeRegressor(random_state=1)\names_model.fit(X.iloc[200:], y.iloc[200:])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3b8aa300e03cb41be19ddc927ab8dadb83f3c62f"},"cell_type":"code","source":"print(\"Making predictions for the following 5 houses:\")\nprint(X.iloc[200:205])\nprint(\"The predictions are\")\nprint(ames_model.predict(X.iloc[200:205]))\nprint(\"Actual values:\")\nprint(y.iloc[200:205])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4280b2f3993f5610e1001a6db92d0219d1f48a9b"},"cell_type":"markdown","source":"Heck, that's some tasty accuracy!\n\nNow, let's check the total error using Mean Absolute Error"},{"metadata":{"trusted":true,"_uuid":"17288e52c141a49b9833d5732c5948138a14c793"},"cell_type":"code","source":"from sklearn.metrics import mean_absolute_error #As someone who's most experienced with C and java, this line physically hurts me\n\npredicted_home_prices = ames_model.predict(X.iloc[200:])\nmean_absolute_error(y.iloc[200:], predicted_home_prices)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"62896449ad7eba9082c67bd61e83c63b82f4e705"},"cell_type":"markdown","source":"Oooooooowweeeeeeeee! perfect! except, that's probably not accurate. This model has been overfitted, and won't be accurate for any new observations. let's test this out using the 200 we left out of our first trial:"},{"metadata":{"trusted":true,"_uuid":"0f59b72e72448175a31d4cf3a637586f5efcdada"},"cell_type":"code","source":"predicted_home_prices = ames_model.predict(X[:200])\nmean_absolute_error(y[:200], predicted_home_prices)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3df394813cdbd453486497e2fcff2175a705700b"},"cell_type":"markdown","source":"That's closer to what I would expect. while a model can be fitted to be (basially) perfectly accurate on the data you train them on, it's worthless on new data. We can 'avoid' this using the train-test split."},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"48fd523e1ae8fdcf4fba9dd5c8aa75a7324dd9d9"},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\ntrain_X, val_X, train_y, val_y = train_test_split(X, y, random_state = 0)\names_model = DecisionTreeRegressor()\names_model.fit(train_X, train_y)\n\n# get predicted prices on validation data\nval_predictions = ames_model.predict(val_X)\nprint(mean_absolute_error(val_y, val_predictions))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7d0502c6758cd3dd9e8fd5359ce0fa689ed17c82"},"cell_type":"markdown","source":"This may seems just as bad, but now we know it's our model that's inaccurate, not that we've overfitted the model. So now let's make it better! Here's a copypasted way to compare changes to our model."},{"metadata":{"trusted":true,"_uuid":"3cecd36fcf98f37c0eab7557c91bbf2ba5df0b6d"},"cell_type":"code","source":"def get_mae(max_leaf_nodes, train_X, val_X, train_y, val_y):\n    model = DecisionTreeRegressor(max_leaf_nodes=max_leaf_nodes, random_state=0)\n    model.fit(train_X, train_y)\n    preds_val = model.predict(val_X)\n    mae = mean_absolute_error(val_y, preds_val)\n    return(mae)\n\nfor max_leaf_nodes in [5, 50, 500, 5000]:\n    my_mae = get_mae(max_leaf_nodes, train_X, val_X, train_y, val_y)\n    print(\"Max leaf nodes: %d  \\t\\t Mean Absolute Error:  %d\" %(max_leaf_nodes, my_mae))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fac5e3e094b13f2aa565bbc8262a72e426ada46a"},"cell_type":"markdown","source":"With this, we see that having 50 maximum leaf nodes is the most accurate model. the max leaf nodes is called a 'hyperparameter', it's something that we can change to get a better fitted model. The value that the hyperparameters should be at is different for every dataset.\n\nWe'll come back to hyperparameters later. let's improve our data set now! First let's estimate those NaN values we dropped with a tool called imputation."},{"metadata":{"trusted":true,"_uuid":"f1e9290f15fba2aaef75355f01c1ab4a20205277"},"cell_type":"code","source":"from sklearn.preprocessing.imputation import Imputer\n\n# make copy to avoid changing original data (when Imputing)\nnew_data = training_data.copy().select_dtypes(exclude='object')\n\n# make new columns indicating what will be imputed\ncols_with_missing = (col for col in new_data.columns \n                                 if new_data[col].isnull().any())\nfor col in cols_with_missing:\n    new_data[col + '_was_missing'] = new_data[col].isnull()\n\n# Imputation\nmy_imputer = Imputer()\nnew_data = pd.DataFrame(my_imputer.fit_transform(new_data))\n\nnew_y = new_data[40] # The imputer added two columns, I don't know why...\nnew_data = new_data.drop(labels=40,axis=1)\ntrain_X, val_X, train_y, val_y = train_test_split(X, y, random_state = 0)\n\nfor max_leaf_nodes in [5, 50, 500, 5000]:\n    my_mae = get_mae(max_leaf_nodes, train_X, val_X, train_y, val_y)\n    print(\"Max leaf nodes: %d  \\t\\t Mean Absolute Error:  %d\" %(max_leaf_nodes, my_mae))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9cc56d3c6cc9e5449b06b6ab6ce392e689d7015e"},"cell_type":"markdown","source":"Hmm, that didn't do much.  Let's try including the text (catagorical) data."},{"metadata":{"trusted":true,"_uuid":"636867daf1e877770c5d18357a3009a4813fd042"},"cell_type":"code","source":"y = training_data.SalePrice\nX = training_data.drop(labels=\"SalePrice\", axis=1)\none_hot_encoded_training_predictors = pd.get_dummies(X)\nnew_data = one_hot_encoded_training_predictors.copy()\n\ncols_with_missing = (col for col in new_data.columns \n                                 if new_data[col].isnull().any())\nfor col in cols_with_missing:\n    new_data[col + '_was_missing'] = new_data[col].isnull()\nnew_data = pd.DataFrame(my_imputer.fit_transform(new_data))\n\ntrain_X, val_X, train_y, val_y = train_test_split(new_data, y, random_state = 0)\nfor max_leaf_nodes in [5, 50, 500, 5000]:\n    my_mae = get_mae(max_leaf_nodes, train_X, val_X, train_y, val_y)\n    print(\"Max leaf nodes: %d  \\t\\t Mean Absolute Error:  %d\" %(max_leaf_nodes, my_mae))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4adbfc5e817ee7cd67d2d88eab5e14afe7db48d9"},"cell_type":"markdown","source":"Hmm, still not good, let's try a different model."},{"metadata":{"trusted":true,"_uuid":"082470a5c4a4f76b66e464b951c4af6425f0b4bc"},"cell_type":"code","source":"from xgboost import XGBRegressor\n\nmy_model = XGBRegressor(n_estimators=100, learning_rate=0.02)\nmy_model.fit(train_X, train_y, early_stopping_rounds=2, \n             eval_set=[(val_X, val_y)], verbose=False)\n\npredictions = my_model.predict(val_X)\n\nprint(\"Mean Absolute Error : \" + str(mean_absolute_error(predictions, val_y)))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"53c697aaf4dc20cb3353e09135711652503b3baf"},"cell_type":"markdown","source":"Now there we go! cut the error down to 30,000! n_estimators, learning_rate, and early_stopping_rounds are hyperparameters. play around with these values and see if you can improve the model! I got it down below 17,000, so that's your target!\n\nIf you want to try something more advanced, look through these tutorials :https://www.kaggle.com/c/house-prices-advanced-regression-techniques#tutorials"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}
